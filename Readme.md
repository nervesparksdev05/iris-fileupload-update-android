<<<<<<< HEAD
# Iris - Offline AI Chat for Android

<div align="center">

[![Get it on Google Play](https://img.shields.io/badge/Get%20it%20on-Google%20Play-blue?style=for-the-badge&logo=google-play)](https://play.google.com/store/apps/details?id=com.nervesparks.irisGPT&hl=en_IN)
[![GitHub release](https://img.shields.io/github/v/release/nerve-sparks/iris_android?style=for-the-badge)](https://github.com/nerve-sparks/iris_android/releases)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

**Run powerful AI models completely offline on your Android device**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Screenshots](#screenshots) â€¢ [Documentation](#documentation) â€¢ [Contributing](#contributing)

</div>

---

## ğŸ“– Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Screenshots](#screenshots)
- [Installation](#installation)
    - [From Google Play](#from-google-play)
    - [From GitHub Releases](#from-github-releases)
    - [Initial Setup](#initial-setup)
- [Supported Models](#supported-models)
- [RAG & Document Processing](#rag--document-processing)
- [Performance Optimization](#performance-optimization)
- [Building from Source](#building-from-source)
- [Adding Custom Models](#adding-custom-models)
- [Configuration](#configuration)
- [Technical Details](#technical-details)
- [Contributing](#contributing)
- [Troubleshooting](#troubleshooting)
- [License](#license)
- [Contact](#contact)

---

## ğŸŒŸ Overview

**Iris** is a powerful, privacy-focused AI chat application for Android that runs **completely offline**. Built on [llama.cpp](https://github.com/ggerganov/llama.cpp), Iris brings state-of-the-art language models directly to your device without requiring an internet connection or sending your data to external servers.

### Why Iris?

- ğŸ”’ **100% Private** - All processing happens on your device
- ğŸ“´ **Fully Offline** - Works in airplane mode after initial setup
- ğŸ¯ **No API Costs** - Free to use with unlimited messages
- ğŸš€ **Multiple Models** - Support for various GGUF models
- ğŸ“„ **RAG Support** - Upload and query your own documents
- âš¡ **Optimized** - Native C++ backend for maximum performance

---

## âœ¨ Features

### Core Features

- **ğŸ”Œ Offline Operation**
    - Run completely offline after downloading models
    - No internet required for inference
    - Works in airplane mode

- **ğŸ¤– Multiple AI Models**
    - Support for GGUF format models
    - Download directly from Hugging Face
    - Switch between models easily
    - Set default model for automatic loading

- **ğŸ“š Document Processing (RAG)**
    - Upload PDF, DOCX, TXT, and other documents
    - Semantic search across your documents
    - Ask questions about uploaded content
    - Document-only mode for accurate citations
    - Automatic embedding and indexing

- **ğŸ¨ User Experience**
    - Clean, modern Material Design 3 UI
    - Dark mode optimized
    - Smooth animations and transitions
    - Copy/share responses easily

### Advanced Features

- **âš™ï¸ Customizable Inference**
    - Adjust `n_threads` for CPU utilization
    - Configure `top_k`, `top_p`, and `temperature`
    - Fine-tune for speed vs quality trade-offs

- **ğŸ¤ Voice Features**
    - Text-to-Speech (TTS) for AI responses
    - Speech-to-Text (STT) for voice input
    - Built-in Android TTS/STT integration

- **ğŸ“Š Model Management**
    - Download and manage chat models
    - Separate embedding model management
    - View model sizes and status
    - Delete models to free up space

- **ğŸ”§ Developer-Friendly**
    - Open source and transparent
    - Easy to extend and modify
    - Active development and updates

---

## ğŸ“± Screenshots

![WhatsApp Image 2026-01-13 at 5.29.56 PM.jpeg](../../../Pictures/nervesparks/WhatsApp%20Image%202026-01-13%20at%205.29.56%20PM.jpeg)
![WhatsApp Image 2026-01-13 at 5.29.57 PM (1).jpeg](../../../Pictures/nervesparks/WhatsApp%20Image%202026-01-13%20at%205.29.57%20PM%20%281%29.jpeg)
![WhatsApp Image 2026-01-13 at 5.29.57 PM.jpeg](../../../Pictures/nervesparks/WhatsApp%20Image%202026-01-13%20at%205.29.57%20PM.jpeg)
![WhatsApp Image 2026-01-13 at 5.29.58 PM (1).jpeg](../../../Pictures/nervesparks/WhatsApp%20Image%202026-01-13%20at%205.29.58%20PM%20%281%29.jpeg)
![WhatsApp Image 2026-01-13 at 5.29.58 PM.jpeg](../../../Pictures/nervesparks/WhatsApp%20Image%202026-01-13%20at%205.29.58%20PM.jpeg)

---

## ğŸ“¥ Installation

### From Google Play

The easiest way to install Iris:

[![Get it on Google Play](https://play.google.com/intl/en_us/badges/static/images/badges/en_badge_web_generic.png)](https://play.google.com/store/apps/details?id=com.nervesparks.irisGPT&hl=en_IN)

### From GitHub Releases

1. Navigate to the [Releases page](https://github.com/nerve-sparks/iris_android/releases)
2. Download the latest `.apk` file
3. Install the APK on your Android device
    - You may need to enable "Install from Unknown Sources" in your device settings

### Initial Setup

After installing the app:

1. **Open Iris** and navigate to the **Models** screen
2. **Download a Chat Model** (required)
    - Choose based on your device's capabilities
    - Recommended: Start with TinyLlama or Llama 3.2-1B for testing
3. **Download an Embedding Model** (optional but recommended)
    - Required for document processing features
    - Recommended: `bge-small-en-v1.5-q4_k_m.gguf` (~25MB)
4. **Wait for downloads to complete**
5. **Start chatting!** The default model will load automatically

> **Note:** Models are large files (ranging from ~600MB to 2GB+). Ensure you have sufficient storage space and a stable connection for initial downloads.

---

## ğŸ¤– Supported Models

### Recommended Chat Models

| Model | Size | Quantization | Speed | Quality | Best For |
|-------|------|--------------|-------|---------|----------|
| **TinyLlama 1.1B** | ~600MB | Q4_K_M | âš¡âš¡âš¡ | â­â­ | Quick responses, low-end devices |
| **Llama 3.2-1B** | ~800MB | Q6_K_L | âš¡âš¡ | â­â­â­ | Balanced performance |
| **Llama 3.2-3B** | ~1.9GB | Q4_K_L | âš¡ | â­â­â­â­ | Best quality for most devices |
| **StableLM 1.6B** | ~1GB | Q4_K_M | âš¡âš¡ | â­â­â­ | Good balance |

### Embedding Models

| Model | Size | Best For |
|-------|------|----------|
| **bge-small-en-v1.5** | ~25MB | Document processing, fast retrieval |
| **mxbai-embed-large-v1** | ~670MB | High-quality embeddings, better accuracy |

### Model Selection Guide

**For Low-End Devices (2-4GB RAM):**
- TinyLlama 1.1B (Q4_K_M)
- bge-small-en-v1.5 embedding

**For Mid-Range Devices (4-6GB RAM):**
- Llama 3.2-1B (Q6_K_L) or StableLM 1.6B
- bge-small-en-v1.5 embedding

**For High-End Devices (6GB+ RAM):**
- Llama 3.2-3B (Q4_K_L)
- mxbai-embed-large-v1 embedding

---

## ğŸ“„ RAG & Document Processing

### What is RAG?

**Retrieval-Augmented Generation (RAG)** allows Iris to answer questions based on your uploaded documents. The system:

1. **Indexes** your documents using embeddings
2. **Retrieves** relevant sections when you ask questions
3. **Generates** answers using only the document content
4. **Cites** sources with document name and section numbers

### Supported Document Formats

- âœ… PDF (`.pdf`)
- âœ… Microsoft Word (`.docx`)
- âœ… Text Files (`.txt`)
- âœ… Markdown (`.md`)
- âœ… CSV (`.csv`)

### How to Use RAG

1. **Download an Embedding Model** (required)
    - Go to **Models â†’ Embedding Models**
    - Download `bge-small-en-v1.5-q4_k_m.gguf`

2. **Upload Documents**
    - Go to **Documents** screen
    - Tap **Upload Document**
    - Select one or more files
    - Wait for indexing to complete

3. **Ask Questions**
    - Type your question in the chat
    - Iris will automatically search your documents
    - Answers include citations like `[Document Name Â§3]`

### Document Mode Behavior

When documents are uploaded, Iris operates in **Document-Only Mode**:

- âœ… Answers **ONLY** from uploaded documents
- âœ… Cites sources for all claims
- âœ… Says "not found" if information isn't in documents
- âŒ Does **NOT** use general knowledge or training data

This ensures:
- ğŸ“Œ **Accuracy** - No hallucinations or made-up information
- ğŸ¯ **Reliability** - Answers are always grounded in your documents
- ğŸ” **Transparency** - Every claim is cited

### RAG Tips

**Best Practices:**
- Upload well-formatted documents (PDFs work best)
- Keep documents under 100MB each
- Use descriptive filenames
- Ask specific questions about document content

**Limitations:**
- Scanned PDFs (images) may not work well
- Very large documents may take time to index
- Model quality affects answer quality

---

## âš¡ Performance Optimization

### Device Requirements

**Minimum:**
- Android 8.0 (API 26) or higher
- 2GB RAM
- 2GB free storage

**Recommended:**
- Android 11.0 or higher
- 4GB+ RAM
- 5GB+ free storage

### Optimization Tips

#### For Faster Responses:

1. **Use Smaller Models**
    - TinyLlama 1.1B is 3-5x faster than 3B models
    - Lower quantizations (Q4) are faster than higher (Q6)

2. **Adjust Thread Count**
    - Settings â†’ Parameters â†’ `n_threads`
    - Use 4-6 threads for most devices
    - Higher = faster but more battery usage

3. **Lower Quality Settings**
    - Reduce `top_k` to 20-30
    - Lower `temperature` to 0.5-0.7

#### For Better Quality:

1. **Use Larger Models**
    - Llama 3.2-3B provides significantly better responses
    - Higher quantizations (Q6) have better accuracy

2. **Increase Context**
    - Keep more conversation history
    - Use higher `top_p` (0.9-0.95)

3. **Optimize Temperature**
    - Higher temperature (0.7-0.9) = more creative
    - Lower temperature (0.3-0.5) = more focused/factual

### Battery Life

**Recommendations:**
- Use smaller models for extended use
- Close the app when not in use
- Reduce thread count if overheating occurs
- Enable battery optimization for the app

---

## ğŸ”¨ Building from Source

### Prerequisites

- **Android Studio** (latest version recommended)
- **Git**
- **Android SDK** (API 26+)
- **NDK** (for native C++ compilation)

### Build Steps

1. **Clone the repository:**
=======
<h2>
  <a href="https://play.google.com/store/apps/details?id=com.nervesparks.irisGPT&hl=en_IN" style="color: white;">
    Iris
  </a>
</h2>

## Overview

**Iris** is a fully offline Android assistant built on **llama.cpp**, designed for privacy-first, on-device AI chat and **offline RAG (Retrieval-Augmented Generation)**.

- **Runs completely offline** after installing and downloading/copying models
- **Private by design**: inference and retrieval happen on-device
- **Extensible**: download GGUF models from Hugging Face (optional; requires internet only for download)
- **Offline RAG**: upload documents â†’ local indexing â†’ answers grounded only in your files

> âš ï¸ Important: Iris does not require the internet to run once models are available locally. Any internet usage is only for optional model downloads.

---

## Key Features

### Offline Chat (llama.cpp)
- On-device LLM chat via llama.cpp
- Adjustable parameters (threads, temperature, top_k, top_p, etc.)
- Default model selection
- Text-to-Speech (TTS) and Speech-to-Text (STT) support

### Offline RAG Assistant (Local-only)
- Upload documents from your device
- Text extraction and chunking happens locally
- Embeddings computed on-device (offline)
- Retrieval is local: answers are generated using only your uploaded documents
- Index and documents are stored in app-local storage

**No cloud. No external database. No remote calls.**

---

## Screenshots

<div style="display: flex; gap: 15px; justify-content: center; flex-wrap: wrap;">
  <div style="text-align: center; width: 200px;">
    <img src="./images/main_screen.png" alt="Main Screen Screenshot" width="200">
    <p><strong>Main Screen</strong></p>
    <p>Main interface where users access core features.</p>
  </div>
  <div style="text-align: center; width: 200px;">
    <img src="./images/chat_screen.png" alt="Chat Screen Screenshot" width="200">
    <p><strong>Chat Screen</strong></p>
    <p>Offline chat experience powered by llama.cpp.</p>
  </div>
  <div style="text-align: center; width: 200px;">
    <img src="./images/settings_screen.png" alt="Settings Screen Screenshot" width="200">
    <p><strong>Settings Screen</strong></p>
    <p>Customize preferences, parameters, and defaults.</p>
  </div>
  <div style="text-align: center; width: 200px;">
    <img src="./images/models_screen.png" alt="Models Screen Screenshot" width="200">
    <p><strong>Models Screen</strong></p>
    <p>Manage local GGUF models, download or select defaults.</p>
  </div>
  <div style="text-align: center; width: 200px;">
    <img src="./images/parameters_screen.png" alt="Parameters Screen Screenshot" width="200">
    <p><strong>Parameters Screen</strong></p>
    <p>Tune performance and response behavior.</p>
  </div>
</div>

---

## Installation

### Google Play
- [Get Iris on Google Play](https://play.google.com/store/apps/details?id=com.nervesparks.irisGPT&hl=en_IN)

### GitHub Releases
- Go to **Releases**: https://github.com/nerve-sparks/iris_android/releases  
- Download the APK
- Install on your device

---

## Getting Started

1. **Install the app**
2. **Add an LLM GGUF model**
   - Download from the in-app models screen (requires internet once), or
   - Copy a GGUF model file into the app storage
3. **(Optional) Use Offline RAG**
   - Upload documents inside the app
   - Wait for indexing to complete
   - Ask questions â€” responses will be grounded only in those documents

---

## Optimizing Your Experience

Performance depends heavily on model size and your device compute.

- **Smaller models** â†’ faster responses, lower memory usage, slightly lower quality
- **Larger models** â†’ higher quality, more memory/compute, slower output

**Recommendation:** start with a small/medium GGUF and increase based on device capability.
>>>>>>> d55e555f6092839146741500717606069d8ea533

---

## Disclaimer

- Iris may produce **incorrect or incomplete answers**, depending on model limitations and query complexity.
- For RAG: answers depend on the **quality and content** of uploaded documents.

---

## Build From Source

### Prerequisites
- Android Studio (latest stable)
- Android NDK as required by the project

### Clone Repositories

Clone Iris:
```bash
git clone https://github.com/nerve-sparks/iris_android.git
<<<<<<< HEAD
cd iris_android
```

2. **Clone llama.cpp dependency:**

```bash
cd ..
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
git checkout 1f922254f0c984a8fb9fbaa0c390d7ffae49aedb
cd ../iris_android
```

3. **Open in Android Studio:**
    - File â†’ Open â†’ Select `iris_android` folder
    - Wait for Gradle sync to complete

4. **Connect your device:**

   **Option A: USB Debugging**
    - Enable Developer Options on your device
    - Enable USB Debugging
    - Connect via USB
    - Authorize the computer

   **Option B: Wireless Debugging (Android 11+)**
    - Enable Developer Options
    - Enable Wireless Debugging
    - In Android Studio: Run â†’ Pair Devices Using Wi-Fi
    - Scan the QR code shown on your device
    - Both devices must be on the same network

5. **Build and Run:**
    - Select your device from the dropdown
    - Click the **Run** button (â–¶ï¸)
    - Wait for compilation and installation

6. **Download Models:**
    - Open the app
    - Navigate to **Models**
    - Download at least one chat model
    - (Optional) Download an embedding model

### Project Structure

```
iris_android/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”‚   â”œâ”€â”€ java/com/nervesparks/iris/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data/                    # Data layer
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ UserPreferencesRepository.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ docs/                    # Document handling
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentTextExtractor
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DocumentUriPermission
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ irisapp/                 # App initialization
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ IrisApp
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ServiceLocator
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag/                     # RAG system
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ embed/               # Embedding generation
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Embedder
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ LlamaCppEmbedder
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingest/              # Document processing
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Chunker
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TextNormalize
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval/           # Search & retrieval
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ VectorSearch
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ storage/             # Data persistence
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ LocalRagStore.kt
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ util/                # Utilities
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ FloatPacking
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ worker/              # Background tasks
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ IndexDocumentWorker
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RagModels.kt
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ RagRepository
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ui/                      # UI components
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ components/          # Reusable components
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ screens/             # App screens
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ theme/               # Material theming
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MainActivity.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ MainViewModel.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ cpp/                         # Native C++ code
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ llama-android.cpp        # JNI bridge to llama.cpp
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ CMakeLists.txt           # C++ build config
â”‚   â”‚   â”‚   â””â”€â”€ res/                         # Android resources
â”‚   â”‚   â”‚       â”œâ”€â”€ drawable/                # Images & icons
â”‚   â”‚   â”‚       â”œâ”€â”€ layout/                  # XML layouts (if any)
â”‚   â”‚   â”‚       â”œâ”€â”€ values/                  # Strings, colors, themes
â”‚   â”‚   â”‚       â””â”€â”€ xml/                     # Preferences & config
â”‚   â”œâ”€â”€ build.gradle                         # App-level build config
â”‚   â””â”€â”€ CMakeLists.txt                       # Root C++ build config
â”œâ”€â”€ gradle/                                  # Gradle wrapper
â”œâ”€â”€ llama.cpp/                               # llama.cpp submodule
â”œâ”€â”€ build.gradle                             # Project-level build config
â”œâ”€â”€ settings.gradle                          # Project settings
â”œâ”€â”€ README.md                                # This file
â””â”€â”€ LICENSE                                  # License file
```

---

## ğŸ¯ Adding Custom Models

### Adding a Chat Model (GGUF)

1. **Find the model on Hugging Face**
    - Browse [Hugging Face](https://huggingface.co/models)
    - Look for GGUF format models

2. **Get the direct download URL:**
    - âœ… Correct format: `https://huggingface.co/<user>/<repo>/resolve/main/<file>.gguf?download=true`
    - âŒ Wrong format: `https://huggingface.co/<user>/<repo>/blob/main/<file>.gguf`

3. **Add to `MainViewModel.kt`:**

```kotlin
var allModels by mutableStateOf(
    listOf(
        // ... existing models ...
        mapOf(
            "name" to "your-model-name.gguf",
            "source" to "https://huggingface.co/user/repo/resolve/main/your-model-name.gguf?download=true",
            "destination" to "your-model-name.gguf"
        )
    )
)
```

### Adding an Embedding Model

Add to the `embeddingModels` list:

```kotlin
var embeddingModels by mutableStateOf(
    listOf(
        // ... existing models ...
        mapOf(
            "name" to "your-embedding-model.gguf",
            "source" to "https://huggingface.co/user/repo/resolve/main/your-embedding-model.gguf?download=true",
            "destination" to "your-embedding-model.gguf",
            "size" to "~50MB",
            "description" to "Description of your embedding model"
        )
    )
)
```

### Testing Custom Models

After adding:
1. Rebuild the app
2. The model appears in **Suggested Models**
3. Download and test thoroughly
4. Share your findings with the community!

---

## âš™ï¸ Configuration

### Inference Parameters

Configure in **Settings â†’ Parameters**:

| Parameter | Range | Default | Description |
|-----------|-------|---------|-------------|
| **Temperature** | 0.0 - 2.0 | 0.7 | Controls randomness (lower = more focused, higher = more creative) |
| **Top P** | 0.0 - 1.0 | 0.9 | Nucleus sampling threshold |
| **Top K** | 1 - 100 | 40 | Limits vocabulary to top K tokens |
| **Threads** | 1 - 8 | 4 | CPU threads for inference (more = faster but more battery) |

### Recommended Configurations

**For Factual Q&A / Documents:**
```
Temperature: 0.3
Top P: 0.9
Top K: 30
```

**For Creative Writing:**
```
Temperature: 0.8
Top P: 0.95
Top K: 50
```

**For Balanced Performance:**
```
Temperature: 0.7
Top P: 0.9
Top K: 40
```

---

## ğŸ”§ Technical Details

### Architecture

- **Frontend:** Jetpack Compose (Material Design 3)
- **Backend:** llama.cpp (C++ native)
- **RAG System:** Custom implementation with vector search
- **Embedding:** BGE/MxBai models via llama.cpp
- **Document Processing:** Apache POI, PdfBox, iText
- **Storage:** Local file system with JSONL format

### Key Technologies

- **Kotlin** - Primary language
- **Jetpack Compose** - Modern UI toolkit
- **llama.cpp** - High-performance LLM inference
- **Coroutines** - Asynchronous operations
- **Flow** - Reactive data streams
- **WorkManager** - Background document indexing

### RAG Implementation

> **ğŸ“š For complete RAG system documentation, see [RAG_ARCHITECTURE.md](RAG_ARCHITECTURE.md)**

**Architecture Overview:**

The RAG system is organized into modular components for maintainability and scalability:

```
com.nervesparks.iris/rag/
â”œâ”€â”€ embed/                      # Embedding generation
â”‚   â”œâ”€â”€ Embedder               # Embedding interface
â”‚   â””â”€â”€ LlamaCppEmbedder       # llama.cpp implementation
â”œâ”€â”€ ingest/                     # Document processing
â”‚   â”œâ”€â”€ Chunker                # Text chunking logic
â”‚   â””â”€â”€ TextNormalize          # Text cleanup & normalization
â”œâ”€â”€ retrieval/                  # Search & retrieval
â”‚   â””â”€â”€ VectorSearch           # Similarity search implementation
â”œâ”€â”€ storage/                    # Data persistence
â”‚   â””â”€â”€ LocalRagStore.kt       # File-based storage manager
â”œâ”€â”€ util/                       # Utilities
â”‚   â””â”€â”€ FloatPacking           # Float32 binary serialization
â”œâ”€â”€ worker/                     # Background processing
â”‚   â””â”€â”€ IndexDocumentWorker    # Async document indexing
â”œâ”€â”€ RagModels.kt               # Data models & types
â””â”€â”€ RagRepository              # Main RAG coordinator
```

**Component Details:**

1. **Document Ingestion** (`docs/`)
    - `DocumentTextExtractor` - Extracts text from PDF, DOCX, TXT, CSV
    - `DocumentUriPermission` - Manages Android URI permissions
    - Supports multiple document formats with format-specific extractors

2. **Text Processing** (`rag/ingest/`)
    - `TextNormalize` - Cleans and normalizes extracted text
    - `Chunker` - Splits documents into semantic chunks (900 chars with 250 char overlap)
    - Handles deduplication and quality filtering

3. **Embedding** (`rag/embed/`)
    - `Embedder` - Abstract interface for embedding models
    - `LlamaCppEmbedder` - Concrete implementation using llama.cpp
    - Converts text chunks to dense vector representations

4. **Vector Storage** (`rag/storage/`)
    - `LocalRagStore` - Manages document metadata and embeddings
    - File-based storage with atomic writes
    - Efficient binary format for embeddings

5. **Retrieval** (`rag/retrieval/`)
    - `VectorSearch` - Implements dot product similarity search
    - Fast in-memory search across document chunks
    - Configurable top-k and score threshold

6. **Background Processing** (`rag/worker/`)
    - `IndexDocumentWorker` - WorkManager-based async indexing
    - Handles long-running document processing
    - Progress tracking and error handling

7. **Coordination** (`RagRepository`)
    - Main entry point for RAG operations
    - Manages document lifecycle (add, remove, query)
    - Coordinates between all components

**Storage Format:**
```
/data/data/com.nervesparks.iris/files/
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ docs/
â”‚       â””â”€â”€ {docId}/
â”‚           â”œâ”€â”€ meta.json           # Document metadata (name, status, etc.)
â”‚           â”œâ”€â”€ chunks.jsonl        # Text chunks (one per line)
â”‚           â””â”€â”€ embeddings.bin      # Binary float32 vectors (LE)
â””â”€â”€ user_docs/                      # Temporary upload storage
    â””â”€â”€ {timestamp}_{uuid}_{filename}
```

**Data Flow:**

1. **Indexing Pipeline:**
   ```
   User Upload â†’ DocumentUriPermission â†’ Copy to user_docs/
   â†’ IndexDocumentWorker â†’ DocumentTextExtractor â†’ TextNormalize
   â†’ Chunker â†’ LlamaCppEmbedder â†’ LocalRagStore
   â†’ Update Status to READY
   ```

2. **Retrieval Pipeline:**
   ```
   User Query â†’ RagRepository.retrieve() â†’ LlamaCppEmbedder (query)
   â†’ LocalRagStore (load docs) â†’ VectorSearch (similarity)
   â†’ RagRepository.buildContextBlock() â†’ Inject into LLM prompt
   ```

**Key Design Decisions:**

- **File-based storage** for simplicity and debugging
- **Binary embeddings** for space efficiency (float32 little-endian)
- **JSONL for chunks** for easy streaming and inspection
- **WorkManager** for reliable background processing
- **Atomic writes** to prevent corruption on crashes

**Component Interaction Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Interface                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Upload Document                   â”‚ Ask Question
             â–¼                                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RagRepository     â”‚              â”‚ MainViewModel   â”‚
    â”‚  .addDocuments()   â”‚              â”‚ .send()         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                   â”‚
             â–¼                                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ IndexDocumentWorker â”‚            â”‚ RagRepository    â”‚
    â”‚ (Background)        â”‚            â”‚ .retrieve()      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                   â”‚
             â”‚ 1. Extract Text                   â”‚ 1. Embed Query
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                â”‚
             â”‚ DocumentTextExtractor             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
             â”‚                                   â”‚ LlamaCppEmbedder
             â”‚ 2. Normalize                      â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                â”‚ 2. Load Docs
             â”‚ TextNormalize                     â”‚
             â”‚                                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
             â”‚ 3. Chunk                          â”‚ LocalRagStore
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                â”‚
             â”‚ Chunker                           â”‚ 3. Search
             â”‚                                   â”‚
             â”‚ 4. Embed Chunks                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                â”‚ VectorSearch
             â”‚ LlamaCppEmbedder                  â”‚
             â”‚                                   â”‚ 4. Build Context
             â”‚ 5. Store                          â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ LocalRagStore                              â”‚
             â”‚                                            â–¼
             â–¼                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚ LLM Generation  â”‚
    â”‚ Document READY  â”‚                         â”‚ with Citations  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Characteristics

**Model Loading:**
- TinyLlama 1.1B: ~2-4 seconds
- Llama 3.2-3B: ~5-10 seconds

**Inference Speed (tokens/sec):**
- TinyLlama on mid-range device: 10-15 t/s
- Llama 3.2-3B on mid-range device: 3-6 t/s

**Document Indexing:**
- Speed: ~5-10 pages/second
- Depends on embedding model and device

---

## ğŸ¤ Contributing

We welcome contributions from the community! Here's how you can help:

### Ways to Contribute

1. **ğŸ› Report Bugs**
    - Use GitHub Issues
    - Include device info, Android version, and steps to reproduce
    - Attach logs if possible

2. **ğŸ’¡ Suggest Features**
    - Open a GitHub Discussion
    - Explain the use case and benefits
    - Share mockups if applicable

3. **ğŸ“ Improve Documentation**
    - Fix typos and unclear sections
    - Add examples and tutorials
    - Translate to other languages

4. **ğŸ’» Submit Code**
    - Follow the contribution workflow below
    - Ensure code quality and testing
    - Update documentation as needed

### Contribution Workflow

1. **Fork the repository**

```bash
git clone https://github.com/YOUR_USERNAME/iris_android.git
cd iris_android
```

2. **Create a feature branch**

```bash
git checkout -b feature/amazing-feature
```

3. **Make your changes**
    - Follow Kotlin coding conventions
    - Write clear commit messages
    - Add tests if applicable

4. **Test thoroughly**
    - Build and run on multiple devices
    - Test edge cases
    - Verify no regressions

5. **Commit your changes**

```bash
git commit -m "Add amazing feature that does X"
```

6. **Push to your fork**

```bash
git push origin feature/amazing-feature
```

7. **Open a Pull Request**
    - Describe your changes clearly
    - Reference related issues
    - Wait for review and address feedback

### Code Style Guidelines

- Follow [Kotlin Coding Conventions](https://kotlinlang.org/docs/coding-conventions.html)
- Use meaningful variable and function names
- Add comments for complex logic
- Keep functions small and focused
- Write self-documenting code

### Commit Message Format

```
<type>: <subject>

<body>

<footer>
```

**Types:**
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, etc.)
- `refactor`: Code refactoring
- `test`: Adding tests
- `chore`: Maintenance tasks

**Example:**
```
feat: Add support for EPUB document format

- Implement EPUB text extraction
- Add EPUB to supported formats list
- Update document picker UI

Closes #123
```

---

## â“ Troubleshooting

### Common Issues

#### App Crashes on Model Load

**Symptoms:** App crashes when loading a model

**Solutions:**
- Ensure sufficient free RAM (close other apps)
- Try a smaller model
- Reduce thread count in settings
- Clear app cache and retry

#### Document Processing Fails

**Symptoms:** "Indexing failed" or "No text extracted"

**Solutions:**
- Ensure embedding model is downloaded
- Check document format is supported
- Try a different document
- Verify document isn't password-protected or corrupted

#### Slow Performance

**Symptoms:** Long wait times for responses

**Solutions:**
- Switch to a smaller model
- Reduce thread count (paradoxically can help on some devices)
- Close background apps
- Restart the device
- Check device isn't overheating

#### Model Download Fails

**Symptoms:** Download stops or shows error

**Solutions:**
- Check internet connection stability
- Ensure sufficient storage space
- Try downloading again
- Use Wi-Fi instead of mobile data
- Clear app cache

#### RAG Not Working

**Symptoms:** App doesn't use document context

**Solutions:**
- Verify embedding model is downloaded and shows "READY"
- Check document status shows "READY" not "INDEXING" or "FAILED"
- Wait for indexing to complete
- Try re-uploading the document
- Check logs for specific errors

### Getting Help

If you're still experiencing issues:

1. **Check GitHub Issues** - Your problem might already be solved
2. **Open a New Issue** - Include:
    - Device model and Android version
    - App version
    - Steps to reproduce
    - Logs (if available)
    - Screenshots
3. **Join Discussions** - Ask the community
4. **Contact Support** - Visit [nervesparks.com](https://www.nervesparks.com)

### Debug Logs

To collect logs for bug reports:

```bash
adb logcat | grep -i "iris\|llama"
```

Or use Android Studio's Logcat window.

---

## ğŸ™ Acknowledgments

Special thanks to:

- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - For the incredible inference engine
- **Hugging Face** - For hosting models and making AI accessible
- **Open Source Community** - For continuous inspiration and support
- **Our Users** - For feedback and support

---

## ğŸ“š Documentation

### Additional Resources

- **[RAG_ARCHITECTURE.md](RAG_ARCHITECTURE.md)** - Comprehensive RAG system documentation
    - Component descriptions
    - Data flow diagrams
    - Performance characteristics
    - Debugging guide

- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Contribution guidelines (if available)
- **[CHANGELOG.md](CHANGELOG.md)** - Version history (if available)

---

## ğŸ—ºï¸ Roadmap

Future plans for Iris:

- [ ] Support for more document formats (EPUB, RTF)
- [ ] Multi-modal support (images in chat)
- [ ] Cloud sync for conversations (optional)
- [ ] Custom system prompts
- [ ] Conversation export/import
- [ ] Plugin system for extensions
- [ ] Voice-first interaction mode
- [ ] Better multi-language support

---

<div align="center">

**Made with â¤ï¸ by [Nerve Sparks](https://www.nervesparks.com)**

â­ Star us on GitHub if you find Iris useful!

</div>
=======
>>>>>>> d55e555f6092839146741500717606069d8ea533
